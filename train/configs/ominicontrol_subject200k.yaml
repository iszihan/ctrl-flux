flux_path: "black-forest-labs/FLUX.1-dev"
image_encoder_path: "openai/clip-vit-large-patch14"
dtype: "bf16"
resume_from_checkpoint: false
cache_latents: false
gradient_checkpointing: true
expname: "omini_subject200k_debug"

dataset:
  type: "subject"
  condition_size:
    - 512
    - 512
  target_size:
    - 512
    - 512
  image_size: 512
  padding: 8
  drop_text_prob: 0.1
  drop_image_prob: 0.1
  drop_last: false
  train_dataloader_num_workers: 4
  test_dataloader_num_workers: 1

train:
  num_train_epochs: 4
  batch_size: 4
  gradient_accumulation_steps: 1
  text_encoder_offload: false
  guidance_scale: 1.0
  independent_condition: false
  optimizer: 
    type: 'Prodigy'
    params:
      lr: 1
      use_bias_correction: true
      safeguard_warmup: true
      weight_decay: 0.01
    max_grad_norm: 1.0
  text_encoder:
    max_sequence_length: 512
  lora:
    name: 'subject'
    config:
      r: 16
      lora_alpha: 16
      init_lora_weights: "gaussian"
      target_modules: "(.*x_embedder|.*(?<!single_)transformer_blocks\\.[0-9]+\\.norm1\\.linear|.*(?<!single_)transformer_blocks\\.[0-9]+\\.attn\\.to_k|.*(?<!single_)transformer_blocks\\.[0-9]+\\.attn\\.to_q|.*(?<!single_)transformer_blocks\\.[0-9]+\\.attn\\.to_v|.*(?<!single_)transformer_blocks\\.[0-9]+\\.attn\\.to_out\\.0|.*(?<!single_)transformer_blocks\\.[0-9]+\\.ff\\.net\\.2|.*single_transformer_blocks\\.[0-9]+\\.norm\\.linear|.*single_transformer_blocks\\.[0-9]+\\.proj_mlp|.*single_transformer_blocks\\.[0-9]+\\.proj_out|.*single_transformer_blocks\\.[0-9]+\\.attn.to_k|.*single_transformer_blocks\\.[0-9]+\\.attn.to_q|.*single_transformer_blocks\\.[0-9]+\\.attn.to_v|.*single_transformer_blocks\\.[0-9]+\\.attn.to_out)"
test:
  seed: 42
  batch_size: 1
  dataloader_num_workers: 1

logging:
  output_dir: "runs"
  logging_dir: "logs"
  logger: "wandb"
  checkpointing_steps: 1
  checkpoints_total_limit: 10
  test_steps: 200
  validation_steps: 500